//
// vm
//

import std.stdio;
import std.atomic;
import heap;
import vm.page;
import vm.space;
import vm.result;
import vm.iovec;
import vm.reaper as reaper;
import vfs.node;
import blk;
import thread as _ : thread;
import process as _ : process;
import sys;
import cpu;
import scheduler;
import platform;
import slab : slab_allocator, slab_cache;

struct memory_manager
{
  usize total_physical_memory;
  usize available_physical_memory;

  page mut *zeropage;

  address_space kernelspace;
  page_allocator pageallocator;
  slab_allocator<region> regionallocator;

  fn instance() -> memory_manager mut &
  {
    static instance = #memory_manager();

    return &instance;
  }

  memory_manager() = default;
  memory_manager(#memory_manager&) = default;
  ~memory_manager() = default;
}

fn mm() -> memory_manager mut &
{
  return &memory_manager::instance;
}

fn pagecache(memory_manager mut &) -> page_cache mut &
{
  extern static thread_local cpu_page_cache = #page_cache();

  return cast<page_cache mut &>(extern("cpu_page_cache@tpoff"));
}

fn regioncache(memory_manager mut &) -> slab_cache<region> mut &
{
  extern static thread_local cpu_region_cache = #slab_cache<region>();

  return cast<slab_cache<region> mut &>(extern("cpu_region_cache@tpoff"));
}

pub fn bootstrap(platform::BootInfo &bootinfo) -> void
{
  std::print("platform usable memory:");
  bootinfo.for_each_usable_memory(fn(var &range) {
    std::print("  ", cast<void*>(range.0), '-', cast<void*>(range.1), " ", (range.1 - range.0));
  });

  bootinfo.for_each_usable_memory(fn(var &range) {
    if (mm.total_physical_memory < range.1)
      mm.total_physical_memory = range.1;
  });

  std::print("total physical memory: ", mm.total_physical_memory);

  var pages = mm.total_physical_memory / PAGE_SIZE;
  var pagetable = cast<page mut *>(heap::early_malloc(bootinfo, pages * sizeof<page>).ptr);

  bootstrap(&mut mm.pageallocator, pagetable, pages);

  bootinfo.for_each_usable_memory(fn(var &range) {
    mm.pageallocator.seed_usable_memory(range.0, range.1);
    mm.available_physical_memory += range.1 - range.0;
  });

  bootinfo.for_each_allocated_memory(fn(var addr) {
    mm.pageallocator.inject_allocated_page(addr);
    mm.available_physical_memory -= PAGE_SIZE;
  });

  var zeropage = heap::early_malloc(bootinfo, PAGE_SIZE);

  std::memset(zeropage.ptr, 0, PAGE_SIZE);

  mm.zeropage = mm.pageallocator.page_from_address(physaddr(bootinfo.hunk_base) + (zeropage - bootinfo.heap_base));
  mm.zeropage.ref();

  std::print("available physical memory: ", mm.available_physical_memory);
}

pub fn available_physical_memory() -> usize
{
  return std::atomic_load(&mm.available_physical_memory);
}

pub fn is_user_address(virtaddr addr) -> bool
{
  return platform::is_user_address(addr);
}

pub fn is_kernel_address(virtaddr addr) -> bool
{
  return platform::is_kernel_address(addr);
}

pub fn page_from_address(physaddr page) -> page mut *
{
  return mm.pageallocator.page_from_address(page);
}

pub fn pages_for_range(physrange range) -> std::mutable_span<page>
{
  return mm.pageallocator.pages_for_range(range);
}

pub fn init_pagetable(page_table mut &pagetable) -> void
{
  pagetable.new(fn() -> platform::physaddr_t {
    return allocate_physical(PAGE_SIZE).addr;
  });
}

pub fn dispose_pagetable(page_table mut &pagetable) -> void
{
  pagetable.dispose(null, fn (void mut *, platform::physaddr_t phys) {
    release_physical_page(mm.pageallocator.page_from_address(phys));
  });
}

pub fn allocate_physical(usize size) -> page_allocation
{
  var allocation = mm.pageallocator.allocate(size);

  for (; allocation.empty; )
  {
    reaper::wake_and_wait();

    allocation = mm.pageallocator.allocate(size);
  }

  std::atomic_sub(&mm.available_physical_memory, allocation.size);

  return allocation;
}

pub fn reservation(usize size) -> void
{
  std::assert(platform::interrupts_disabled);

  if (size <= mm.pagecache.len << PAGE_SHIFT)
    return;

  var allocation = allocate_physical(size);

  rof (var mut &page : allocation)
    mm.pagecache.put(&page);
}

pub fn allocate_physical_page() -> page mut *
{
  var guard = std::lock_guard(&mut cpu::irqlock);

  if (var page = mm.pagecache.get())
    return page;

  var allocation = allocate_physical(8*PAGE_SIZE);

  rof (var mut &page : allocation[1 .. allocation.len])
    mm.pagecache.put(&page);

  return mm.pageallocator.page_from_address(allocation.addr);
}

pub fn release_physical_page(page mut *page) -> void
{
  var guard = std::lock_guard(&mut cpu::irqlock);

  mm.pagecache.put(page);
}

pub fn allocate_physical_pages(usize size) -> page mut *
{
  var pages = null<page mut *>();

  for (var i = 0; i < std::ceil_div(size, PAGE_SIZE); ++i)
  {
    var page = allocate_physical_page();
    page.next = pages;
    pages = page;
  }

  return pages;
}

pub fn release_physical_pages(page mut *pages) -> void
{
  for (var page = pages; page; )
  {
    var next = page.next;

    release_physical_page(page);

    page = next;
  }
}

pub fn release_physical_pages(page_allocation pages) -> void
{
  mm.pageallocator.release(pages);

  std::atomic_add(&mm.available_physical_memory, pages.size);
}

fn ref_physical_pages(physrange range) -> void
{
  for (var addr = range.addr; addr < range.end; addr += PAGE_SIZE)
  {
    mm.pageallocator.page_from_address(addr).ref();
  }
}

fn map_physical_pages(page_table mut &pagetable, virtaddr baseaddr, physrange range, protection prot, mflags flags) -> virtrange
{
  pagetable.map(baseaddr, range, prot, flags, fn () -> platform::physaddr_t {
    return mm.pageallocator.address_for_page(allocate_physical_page());
  });

  return virtrange(baseaddr, range.size);
}

pub fn map_physical_pages(virtaddr baseaddr, physrange range, protection prot) -> virtrange
{
  return map_physical_pages(&mut platform::pagetable, baseaddr, range, prot, mflags::keep);
}

pub fn map_physical_pages(virtaddr baseaddr, page mut *pages, protection prot) -> virtrange
{
  var addr = baseaddr;

  for (var page = pages; page; page = page.next)
  {
    map_physical_pages(&mut platform::pagetable, addr, physrange(mm.pageallocator.address_for_page(page), PAGE_SIZE), prot, mflags::keep);

    addr += PAGE_SIZE;
  }

  return virtrange(baseaddr, addr - baseaddr);
}

fn pagetable_shootdown(page_table &pagetable, virtrange range) -> void
{
  var irqs = platform::disable_interrupts();

  var rendezvous = cast<i32>(sys::cpu_count);

  sys::broadcast_ipi_message(fn [pagetable, range, rendezvous] () -> void {

    if (is_kernel_address(range.addr) || platform::pagetable == pagetable)
    {
      if (range.size <= 32*PAGE_SIZE)
      {
        for (var addr = range.addr; addr < range.end; addr += PAGE_SIZE)
          platform::invlpg(addr);
      }
      else if (is_kernel_address(range.addr))
      {
        platform::flush_global_tlb();
      }
      else if (platform::pagetable == pagetable)
      {
        platform::flush_user_tlb();
      }
    }

    std::atomic_sub(&rendezvous, 1);
  });

  platform::restore_interrupts(irqs);

  while (std::volatile_load(&rendezvous) != 1)
    __relax();
}

fn pagetable_shootdown(virtrange range) -> void
{
  pagetable_shootdown(platform::pagetable, range);
}

fn unmap_physical_pages(page_table mut &pagetable, virtrange range) -> void
{
  pagetable.unmap(range);

  if (is_kernel_address(range.addr) || pagetable.cnt > 1)
  {
    pagetable_shootdown(pagetable, range);
  }
}

pub fn unmap_physical_pages(virtrange range) -> void
{
  unmap_physical_pages(&mut platform::pagetable, range);
}

fn address_space(virtaddr addr) -> address_space mut &
{
  if (is_kernel_address(addr))
    return &mm.kernelspace;

  return &process::current.userspace;
}

fn lock_region(address_space mut &space, virtaddr addr) -> region mut *
{
  for (;;)
  {
    {
      var guard = std::lock_guard(&mut cpu::irqlock, &mut space.lock);

      if (var region = space.find(addr); !region || region.try_lock())
        return region;
    }

    scheduler::sleep_yield();
  }
}

pub fn lock_region(virtaddr addr) -> region mut *
{
  return lock_region(&mut address_space(addr), addr);
}

pub fn create_anonymous_region(address_space mut &space, virtrange range, protection prot, usage use) -> vm::result
{
  var guard = std::lock_guard(&mut cpu::irqlock);

  var region = mm.regioncache.get();

  if (!region)
    region = mm.regioncache.siphon(&mut mm.regionallocator);

  region.range = range;
  region.protection = prot;
  region.usage = use;

  std::atomic_add(&space.total_virtual_size, region.range.size);

  {
    var guard = std::lock_guard(&mut cpu::irqlock, &mut space.lock);

    space.insert(region);
  }

  return ok;
}

pub fn create_anonymous_region(virtrange range, protection prot, usage use) -> vm::result
{
  return create_anonymous_region(&mut address_space(range.addr), range, prot, use);
}

pub fn create_physical_region(address_space mut &space, virtrange range, std::vector<physrange> &physical, protection prot, usage use) -> vm::result
{
  var guard = std::lock_guard(&mut cpu::irqlock);

  var region = mm.regioncache.get();

  if (!region)
    region = mm.regioncache.siphon(&mut mm.regionallocator);

  region.range = range;
  region.protection = prot;
  region.usage = use;

  var addr = range.addr;

  for (var phys : physical)
  {
    var pages = physrange(phys.begin & ~PAGE_MASK, (phys.end + PAGE_MASK) & ~PAGE_MASK);

    if (region.is_foreign)
    {
      map_physical_pages(addr, pages, prot);
    }

    if (!region.is_foreign)
    {
      ref_physical_pages(pages);

      map_physical_pages(addr, pages, protection::readonly);
    }

    region.allocated_physical_pages += pages.size >> PAGE_SHIFT;

    addr += pages.size;
  }

  std::atomic_add(&space.allocated_physical_pages, region.allocated_physical_pages);
  std::atomic_add(&space.total_virtual_size, region.range.size);

  {
    var guard = std::lock_guard(&mut cpu::irqlock, &mut space.lock);

    space.insert(region);
  }

  return ok;
}

pub fn create_physical_region(virtrange range, std::vector<physrange> &physical, protection prot, usage use) -> vm::result
{
  return create_physical_region(&mut address_space(range.addr), range, physical, prot, use);
}

pub fn create_backed_region(address_space mut &space, virtrange range, vfs::node mut *node, u64 offset, protection prot, usage use) -> vm::result
{
  var guard = std::lock_guard(&mut cpu::irqlock);

  var region = mm.regioncache.get();

  if (!region)
    region = mm.regioncache.siphon(&mut mm.regionallocator);

  region.range = range;
  region.protection = prot;
  region.usage = use;
  region.offset = offset;
  region.backing = node;

  std::atomic_add(&space.total_virtual_size, region.range.size);

  {
    var guard = std::lock_guard(&mut cpu::irqlock, &mut space.lock);

    space.insert(region);
  }

  return ok;
}

pub fn create_backed_region(virtrange range, vfs::node mut *node, u64 offset, protection prot, usage use) -> vm::result
{
  return create_backed_region(&mut address_space(range.addr), range, node, offset, prot, use);
}

pub fn set_memory_type(page_table mut &pagetable, address_space mut &space, virtrange range, mtype mtype) -> vm::result
{
  var region = lock_region(&mut space, range.addr);

  if (!region)
    return bad_address;

  pagetable.set_memory_type(range, mtype);

  region.unlock();

  return ok;
}

pub fn set_memory_type(virtrange range, mtype mtype) -> vm::result
{
  return set_memory_type(&mut platform::pagetable, &mut address_space(range.addr), range, mtype);
}

fn mprotect(page_table mut &pagetable, address_space mut &space, virtrange range, protection prot) -> vm::result
{
  var region = lock_region(&mut space, range.addr);

  if (!region)
    return bad_address;

  region.protection = prot;

  if (prot == protection::readonly || prot == protection::executable)
  {
    for (var addr = region.range.addr; addr < region.range.end; addr += PAGE_SIZE)
    {
      if (var mapping = pagetable.query(addr); mapping.present)
        map_physical_pages(&mut pagetable, addr, physrange(mapping.address, PAGE_SIZE), prot, mflags::keep);
    }
  }

  region.unlock();

  return ok;
}

pub fn mprotect(virtrange range, protection prot)
{
  return mprotect(&mut platform::pagetable, &mut address_space(range.addr), range, prot);
}

pub fn mprotect(process mut *process, virtrange range, protection prot)
{
  return mprotect(&mut process.pagetable, &mut process.userspace, range, prot);
}

fn munmap(page_table mut &pagetable, address_space mut &space, virtrange range) -> vm::result
{
  var region = lock_region(&mut space, range.addr);

  if (!region)
    return bad_address;

  var is_kernel_region = is_kernel_address(region.range.addr);

  {
    var guard = std::lock_guard(&mut cpu::irqlock, &mut space.lock);

    space.remove(region);
  }

  if (region.is_foreign)
  {
    pagetable.unmap(region.range);

    if (is_kernel_region || pagetable.cnt > 1)
    {
      pagetable_shootdown(pagetable, region.range);
    }
  }

  if (!region.is_foreign)
  {
    struct state
    {
      usize pgcnt;
      page mut *pages;

      state() = default;
      ~state() = default;
    }

    var state = state();

    pagetable.unmap(region.range, &state, fn (void mut *state, platform::virtaddr_t addr, platform::physaddr_t phys) {
      var state = cast<state mut *>(state);
      var page = mm.pageallocator.page_from_address(phys);

      if (page.unref() == 1)
      {
        page.next = state.pages;
        state.pages = page;
      }

      state.pgcnt += 1;
    });

    if (is_kernel_region || pagetable.cnt > 1)
    {
      pagetable_shootdown(pagetable, region.range);
    }

    release_physical_pages(state.pages);
  }

  std::atomic_sub(&space.allocated_physical_pages, region.allocated_physical_pages);
  std::atomic_sub(&space.total_virtual_size, region.range.size);

  region.unlock();

  var guard = std::lock_guard(&mut cpu::irqlock);

  region.offset = 0;
  region.backing = null;
  region.allocated_physical_pages = 0;

  mm.regioncache.put(region);

  return ok;
}

pub fn munmap(virtrange range) -> vm::result
{
  return munmap(&mut platform::pagetable, &mut address_space(range.addr), range);
}

pub fn munmap(process mut *process, virtrange range) -> vm::result
{
  return munmap(&mut process.pagetable, &mut process.userspace, range);
}

fn lock(page_table mut &pagetable, address_space mut &space, virtrange range, protection prot, std::vector<physrange> mut &result) -> vm::result
{
  var status = result::ok;

  if (range.addr == 0 && range.size == 0)
    return ok;

  var region = lock_region(&mut space, range.addr);

  if (!region)
    status = result::bad_address;

  if (status == result::ok)
  {
    if (!region.is_readable)
      status = result::not_permitted;

    if (prot == protection::readwrite && !region.is_writeable)
      status = result::not_permitted;

    if (!region.contains(range))
      status = result::bad_address;
  }

  if (status == result::ok)
  {
    for (var addr = range.addr; addr < range.end; addr = (addr + PAGE_SIZE) & ~PAGE_MASK)
    {
      var mapping = pagetable.query(addr);

      if (!mapping.present || (prot == protection::readwrite && !mapping.writeable))
      {
        status = fault_region(&mut pagetable, region, addr, prot == protection::readwrite, false);

        if (status != result::ok)
          break;

        mapping = pagetable.query(addr);
      }

      var page = mm.pageallocator.page_from_address(mapping.address);

      var phys = physrange(mapping.address + cast<usize>(addr & PAGE_MASK), std::min((addr + PAGE_SIZE) & ~PAGE_MASK, virtaddr(range.end)) - addr);

      if (!result.empty && result.back.end == phys.begin)
        result.back.super.1 += phys.size;
      else
        result.push_back(phys);

      page.ref();
    }
  }

  if (region)
  {
    region.unlock();
  }

  return status;
}

pub fn lock(virtrange range, protection prot, std::vector<physrange> mut &result)
{
  return lock(&mut platform::pagetable, &mut address_space(range.addr), range, prot, &mut result);
}

pub fn lock(process mut *process, virtrange range, protection prot, vm::iovec mut &result) -> vm::result
{
  result.length += range.size;

  return lock(&mut process.pagetable, &mut process.userspace, range, prot, &mut result.host);
}

pub fn lock(virtrange range, protection prot, vm::iovec mut &result)
{
  result.length += range.size;

  return lock(&mut platform::pagetable, &mut address_space(range.addr), range, prot, &mut result.host);
}

pub fn unlock(physrange range) -> void
{
  for (var phys = range.begin; phys < range.end; phys = (phys + PAGE_SIZE) & ~PAGE_MASK)
  {
    var page = mm.pageallocator.page_from_address(phys);

    if (page.unref() == 1)
      release_physical_page(page);
  }
}

pub fn unlock(std::vector<physrange> &regions) -> void
{
  for (var &range : regions)
    unlock(range);
}

pub fn unlock(vm::iovec mut &iovec) -> void
{
  unlock(iovec.host);

  iovec.length = 0;
  iovec.host.clear();
}

pub fn read_process_memory(process mut *process, virtaddr src, void mut *dst, usize length) -> vm::result
{
  var iovec = vm::iovec();

  if (src == 0 && length == 0)
    return ok;

  if (var result = lock(process, virtrange(src, length), vm::protection::readonly, &mut iovec); !result)
    return result;

  if (var result = vm::memcpy(dst, iovec, 0, length); !result)
    return result;

  return ok;
}

pub fn read_process_memory(virtaddr src, void mut *dst, usize length) -> vm::result
{
  var status = result::ok;

  if (src == 0 && length == 0)
    return ok;

  var region = lock_region(&mut process::current.userspace, src);

  if (!region)
    status = result::bad_address;

  if (status == result::ok)
  {
    if (!region.is_readable)
      status = result::not_permitted;

    if (!region.contains(virtrange(src, length)))
      status = result::bad_address;
  }

  if (status == result::ok)
  {
    platform::disable_memory_protect();

    std::memcpy(dst, src.ptr, length);

    platform::enable_memory_protect();
  }

  if (region)
  {
    region.unlock();
  }

  return status;
}

pub fn write_process_memory(process mut *process, virtaddr dst, void *src, usize length) -> vm::result
{
  var iovec = vm::iovec();

  if (dst == 0 && length == 0)
    return ok;

  if (var result = lock(process, virtrange(dst, length), vm::protection::readwrite, &mut iovec); !result)
    return result;

  if (var result = vm::memcpy(iovec, 0, src, length); !result)
    return result;

  return ok;
}

pub fn write_process_memory(virtaddr dst, void *src, usize length) -> vm::result
{
  var status = result::ok;

  if (dst == 0 && length == 0)
    return ok;

  var region = lock_region(&mut process::current.userspace, dst);

  if (!region)
    status = result::bad_address;

  if (status == result::ok)
  {
    if (!region.is_writeable)
      status = result::not_permitted;

    if (!region.contains(virtrange(dst, length)))
      status = result::bad_address;
  }

  if (status == result::ok)
  {
    platform::disable_memory_protect();

    std::memcpy(dst.ptr, src, length);

    platform::enable_memory_protect();
  }

  if (region)
  {
    region.unlock();
  }

  return status;
}

fn fault_region(page_table mut &pagetable, region mut *region, virtaddr addr, page mut *page, protection prot, mflags mflags) -> void
{
  page.ref();

  map_physical_pages(&mut pagetable, addr & ~PAGE_MASK, physrange(mm.pageallocator.address_for_page(page), PAGE_SIZE), prot, mflags);
}

fn fault_region(page_table mut &pagetable, region mut *region, virtaddr addr, bool write, bool fetch) -> vm::result
{
  var mapping = pagetable.query(addr);

  if (!mapping.present)
  {
    if (region.backing)
    {
      var position = cast<u64>(addr - region.range.addr) + region.offset;

      var block = blk::fetch(region.backing, position);

      if (block.wait(); !block)
        return cast(block.result);

      var page = mm.pageallocator.page_from_address(pagetable.query(virtaddr(block.ptr(position))).address);

      if (fetch)
        fault_region(&mut pagetable, region, addr, page, protection::executable, mflags::keep);
      else
        fault_region(&mut pagetable, region, addr, page, protection::readonly, mflags::keep);
    }
    else
    {
      if (write)
        fault_region(&mut pagetable, region, addr, allocate_physical_page(), protection::readwrite, mflags::zero);
      else
        fault_region(&mut pagetable, region, addr, mm.zeropage, protection::readonly, mflags::keep);
    }

    region.allocated_physical_pages += 1;

    if (is_user_address(addr))
      cpu::increment_user_allocated_pages(1);

    mapping = pagetable.query(addr);
  }

  if (!mapping.writeable && write)
  {
    var page = mm.pageallocator.page_from_address(mapping.address);

    if (region.is_private || page == mm.zeropage)
      fault_region(&mut pagetable, region, addr, allocate_physical_page(), protection::readwrite, mflags::copy);
    else
      fault_region(&mut pagetable, region, addr, page, protection::readwrite, mflags::keep);

    if (page.unref() == 1)
      release_physical_page(page);
  }

  return ok;
}

extern fn page_fault(virtaddr addr, bool write, bool fetch, bool user) -> void
{
  var status = result::ok;
  var mut &space = address_space(addr);

  var region = lock_region(&mut space, addr);

  if (!region)
    status = result::bad_address;

  if (user && is_kernel_address(addr))
    status = result::not_permitted;

  if (status == result::ok)
  {
    if (!region.is_readable)
      status = result::not_permitted;

    if (write && !region.is_writeable)
      status = result::not_permitted;

    if (fetch && !region.is_executable)
      status = result::not_permitted;
  }

  if (status == result::ok)
  {
    status = fault_region(&mut platform::pagetable, region, addr, write, fetch);
  }

  if (region)
  {
    region.unlock();
  }

  if (status != result::ok)
  {
    std::print(*thread::current, "page fault: ", status, " at ", addr);

    platform::disable_memory_protect();

    var rbp = cast<uintptr*>(__asm("mov $0, rbp", "=r"));

    for (var i = 0; i < 32; ++i)
    {
      if (!rbp)
        break;

      if (var mapping = platform::pagetable.query(vm::virtaddr(rbp)); !mapping.present)
        break;

      std::print("  ", cast<void*>(*(rbp + 1)));

      rbp = cast<uintptr*>(*rbp);
    }

    platform::enable_memory_protect();

    if (!user)
      std::panic("unhandled page fault in kernel space");

    process::exit(cast<i32>(status));
  }
}

pub fn reap() -> void
{
  var guard = std::lock_guard(&mut cpu::irqlock);

  var rendezvous = cast<i32>(sys::cpu_count);

  sys::broadcast_ipi_message(fn [rendezvous] () -> void {

    while (var page = null<page mut *>(); page = mm.pagecache.get(); page)
    {
      mm.pageallocator.release(std::mutable_span(page, 1));

      std::atomic_add(&mm.available_physical_memory, PAGE_SIZE);
    }

    while (var region = null<region mut *>(); region = mm.regioncache.get(); region)
    {
      mm.regionallocator.free(region);
    }

    std::atomic_sub(&rendezvous, 1);
  });

  while (var page = null<page mut *>(); page = mm.pagecache.get(); page)
  {
    mm.pageallocator.release(std::mutable_span(page, 1));

    std::atomic_add(&mm.available_physical_memory, PAGE_SIZE);
  }

  while (var region = null<region mut *>(); region = mm.regioncache.get(); region)
  {
    mm.regionallocator.free(region);
  }

  while (std::volatile_load(&rendezvous) != 1)
    __relax();

  reap(&mut mm.regionallocator);
}

pub fn reap(page_table mut &pagetable, vm::address_space mut &space) -> void
{
  var guard = std::lock_guard(&mut cpu::irqlock);

  if (pagetable.cnt != 0)
    return;

  if (space.lock.try_lock())
  {
    space.each(|region| {

      if (region.try_lock())
      {
        if (region.backing && region.is_readonly)
        {
          pagetable.unmap(region.range, region, fn (void mut *region, platform::virtaddr_t addr, platform::physaddr_t phys) {
            var page = mm.pageallocator.page_from_address(phys);

            if (page.unref() == 1)
              std::panic();
          });

          region.allocated_physical_pages = 0;
        }

        region.unlock();
      }
    });

    space.lock.unlock();
  }
}
