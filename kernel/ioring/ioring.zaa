//
// ioring
//

import std.stdlib;
import vm : virtaddr, virtrange;
import io;
import cpu;
import vfs;
import vm.result;
import vfs.result;
import slab : slab_allocator;
import mutex as _ : mutex;
import process as _ : process;
import waitqueue as _ : wait_queue;
import support.rc : Rc;

import ioring.open;
import ioring.dup;
import ioring.stat;
import ioring.read;
import ioring.write;
import ioring.ioctl;
import ioring.select;
import ioring.mkdir;
import ioring.rename;
import ioring.link;
import ioring.symlink;
import ioring.unlink;
import ioring.poll;
import ioring.notify;
import ioring.event;
import ioring.buffer;
import ioring.channel;
import ioring.close;
import ioring.workman;

pub enum ioring_ops
{
  pub const open = 0x01;
  pub const stat = 0x02;
  pub const read = 0x03;
  pub const write = 0x04;
  pub const ioctl = 0x05;
  pub const close = 0x06;
  pub const select = 0x07;
  pub const dup = 0x08;
  pub const dup2 = 0x09;
  pub const mkdir = 0x0a;
  pub const rename = 0x0b;
  pub const link = 0x0c;
  pub const symlink = 0x0d;
  pub const unlink = 0x0e;
  pub const poll_create = 0x10;
  pub const poll_add = 0x11;
  pub const poll_modify = 0x12;
  pub const poll_remove = 0x13;
  pub const poll_wait = 0x14;
  pub const notify_create = 0x15;
  pub const notify_add = 0x16;
  pub const notify_modify = 0x17;
  pub const notify_remove = 0x18;
  pub const event_create = 0x19;
  pub const buffer_create = 0x1a;
  pub const channel_create = 0x1b;
  pub const channel_read = 0x1c;
  pub const channel_write = 0x1d;
  pub const channel_call = 0x1e;
}

pub enum ioring_flags
{
  const fin = 0x1;
}

pub struct ioring_header
{
  u32 sq_head;
  u32 sq_tail;
  u32 sq_mask;
  u32 sq_offset;

  u32 cq_head;
  u32 cq_tail;
  u32 cq_mask;
  u32 cq_offset;

  u8[32] reserved;
}

pub struct ioring_sqe
{
  pub u8 op;
  pub u8 flags;
  pub u16 reserved1;
  pub u32 reserved2;
  pub uintptr[6] args;
  pub uintptr user_data;

  pub ioring_sqe() = default;
  pub ioring_sqe(ioring_sqe&) = default;
  pub ~ioring_sqe() = default;
}

pub struct ioring_cqe
{
  pub u32 flags;
  pub i32 result;
  pub uintptr user_data;

  pub ioring_cqe() = default;
  pub ioring_cqe(ioring_cqe&) = default;
  pub ~ioring_cqe() = default;
}

pub enum result : i32
{
  ok = 0,
  not_permitted = -1,
  interrupted = -4,
  would_block = -11,
  device_busy = -16,
  invalid_argument = -22,
  not_supported = -95,
  overflow = -139,

  should_block = -7001,

  pub fn bool(result code) -> bool
  {
    return code >= ok;
  }

  pub fn result(vm::result result) -> result
  {
    return cast(result);
  }

  pub fn result(vfs::result result) -> result
  {
    return cast(result);
  }

  pub fn result(i32 result) -> result
  {
    return cast(result);
  }

  pub fn =(this mut &, var rhs) -> result mut &
  {
    cast<i32 mut &>(this) = cast(rhs);

    return &this;
  }
}

pub struct ioring_ctx
{
  pub result status;
  pub std::vector<ioring_sqe> entries;
  pub std::vector<io::response> blockers;
  pub u64 wake_time;

  pub uintptr[8] state;

  pub ioring_ctx() = default;
  pub ioring_ctx(ioring_ctx &) = default;
  pub fn =(ioring_ctx mut &, ioring_ctx &) -> ioring_ctx mut & = default;
  pub ~ioring_ctx() = default;
}

pub struct io_ring
{
  pub mutex lock;
  wait_queue waiters;

  usize sq_base;
  usize sq_mask;
  usize cq_base;
  usize cq_mask;

  vm::iovec region;

  pub Rc<process> process;
  pub io_ring mut *sibling;

  pub std::vector<ioring_ctx> pending;

  pub io_ring mut *next;

  pub io::response readable;

  vfs::node node;

  u8[48] reserved;

  fn allocator()
  {
    static allocator = #slab_allocator<io_ring>();

    return &allocator;
  }

  fn create() -> Rc<io_ring>
  {
    return allocator.allocate();
  }

  pub fn node(this mut &) -> Rc<vfs::node>
  {
    return &this.node;
  }

  pub fn open(vfs::node mut *node, vfs::fd mut &fd, u64 flags, u32 mode) -> vfs::result
  {
    return ok;
  }

  pub fn poll(vfs::node mut *node, Rc<io::iocb> mut &readable, Rc<io::iocb> mut &writeable) -> vfs::result
  {
    var mut &ring = *cast<io_ring mut *>(cast<uintptr>(node) - offsetof(io_ring::node));

    readable = ring.readable.cb;

    return ok;
  }

  pub fn read(vfs::node mut *node, vm::iovec &iovec, usize offset, usize length) -> vfs::result
  {
    return not_supported;
  }

  pub fn write(vfs::node mut *node, vm::iovec &iovec, usize offset, usize length) -> vfs::result
  {
    return not_supported;
  }

  pub fn getattr(vfs::node mut *node, vfs::stat mut &stat, u64 mask) -> vfs::result
  {
    var mut &ring = *cast<io_ring mut *>(cast<uintptr>(node) - offsetof(io_ring::node));

    stat.size = cast(ring.region.length);

    return ok;
  }

  pub fn setattr(vfs::node mut *node, vfs::stat &stat, u64 mask) -> vfs::result
  {
    return not_supported;
  }

  pub io_ring()
  {
    vfs::node::new(&node, vfs::node::type::ioring, vfs::node_stream_operations());

    io::initiate(&mut readable.cb, 1);
  }

  pub fn ref(this mut &) -> void
  {
    this.node.ref();
  }

  pub fn unref(this mut &) -> void
  {
    this.node.unref();
  }

  pub ~io_ring() = default;
}

pub fn dump(io_ring &ring) -> void
{
  std::print("[io ring]");

  for (var &ctx : ring.pending)
  {
    std::print("  ", ctx.status, " ", ctx.entries.len);
  }
}

pub fn create_ioring() -> Rc<io_ring>
{
  return io_ring::create();
}

pub fn setup_ioring(Rc<io_ring> mut &ring, process mut *process, virtrange buffer) -> result
{
  std::assert(ring.region.host.empty);

  if (buffer.addr & 63 != 0)
    return invalid_argument;

  if (var result = vm::lock(process, buffer, vm::protection::readwrite, &mut ring.region); !result)
    return result;

  var factor = std::floor_log2((buffer.size - sizeof<ioring_header>) * 2/3 / sizeof<ioring_sqe>);

  ring.sq_mask = (1 << factor) - 1;
  ring.sq_base = sizeof<ioring_header>;
  ring.cq_mask = (2 << factor) - 1;
  ring.cq_base = sizeof<ioring_header> + (1 << factor)*sizeof<ioring_sqe>;

  var header = cast<ioring_header mut *>(ring.region.host[0].addr.ptr);

  header.sq_head = 0;
  header.sq_tail = 0;
  header.sq_mask = cast(ring.sq_mask);
  header.sq_offset = cast(ring.sq_base);

  header.cq_head = 0;
  header.cq_tail = 0;
  header.cq_mask = cast(ring.cq_mask);
  header.cq_offset = cast(ring.cq_base);

  ring.pending.clear();
  ring.pending.resize(1);

  process.lock.lock();

  ring.process = process;
  ring.sibling = std::exchange(&mut process.iorings, &*ring);

  process.lock.unlock();

  ring.ref();

  return ok;
}

pub fn destroy_ioring(vfs::node mut *node) -> result
{
  var ring = cast<io_ring mut *>(cast<uintptr>(node) - offsetof(io_ring::node));

  if (process::current != ring.process)
    return not_permitted;

  shutdown(ring);

  return ok;
}

pub fn execute(io_ring mut &ring, ioring_ctx mut &ctx, ioring_sqe &sqe) -> result
{
  if (ctx.status)
  {
    switch (sqe.op)
    {
      case ioring_ops::open:
        ctx.status = sys_open(ring.process, &mut ctx, cast(sqe.args[0] >> 32 & 0x7fffffff), cast(sqe.args[0] & 0xffffffff), virtaddr(sqe.args[1]), cast(sqe.args[2]), cast(sqe.args[3] & 0xffffffff));

      case ioring_ops::stat:
        ctx.status = sys_stat(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), virtaddr(sqe.args[1]), cast(sqe.args[2]));

      case ioring_ops::read:
        ctx.status = sys_read(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), virtaddr(sqe.args[1]), cast(sqe.args[2]));

      case ioring_ops::write:
        ctx.status = sys_write(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), virtaddr(sqe.args[1]), cast(sqe.args[2]));

      case ioring_ops::ioctl:
        ctx.status = sys_ioctl(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), cast(sqe.args[1] & 0xffffffff), virtaddr(sqe.args[2]), cast(sqe.args[3]));

      case ioring_ops::close:
        ctx.status = sys_close(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff));

      case ioring_ops::select:
        ctx.status = sys_select(ring.process, &mut ctx, virtaddr(sqe.args[0]), cast(sqe.args[1]), cast(sqe.args[2]));

      case ioring_ops::dup:
        ctx.status = sys_dup(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff));

      case ioring_ops::dup2:
        ctx.status = sys_dup2(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), cast(sqe.args[1] & 0x7fffffff));

      case ioring_ops::mkdir:
        ctx.status = sys_mkdir(ring.process, &mut ctx, cast(sqe.args[0] >> 32 & 0x7fffffff), cast(sqe.args[0] & 0xffffffff), virtaddr(sqe.args[1]), cast(sqe.args[2]), cast(sqe.args[3] & 0xffffffff));

      case ioring_ops::rename:
        ctx.status = sys_rename(ring.process, &mut ctx, cast((sqe.args[0] >> 32) & 0x7fffffff), cast(sqe.args[0] & 0xffffffff), virtaddr(sqe.args[1]), cast(sqe.args[2] >> 32 & 0x7fffffff), cast(sqe.args[2] & 0xffffffff), virtaddr(sqe.args[3]), cast(sqe.args[4]));

      case ioring_ops::link:
        ctx.status = sys_link(ring.process, &mut ctx, cast((sqe.args[0] >> 32) & 0x7fffffff), cast(sqe.args[0] & 0xffffffff), virtaddr(sqe.args[1]), cast(sqe.args[2] >> 32 & 0x7fffffff), cast(sqe.args[2] & 0xffffffff), virtaddr(sqe.args[3]), cast(sqe.args[4]));

      case ioring_ops::symlink:
        ctx.status = sys_symlink(ring.process, &mut ctx, cast(sqe.args[0] >> 32 & 0x7fffffff), cast(sqe.args[0] & 0xffffffff), virtaddr(sqe.args[1]), cast(sqe.args[2]), virtaddr(sqe.args[3]), cast(sqe.args[4]));

      case ioring_ops::unlink:
        ctx.status = sys_unlink(ring.process, &mut ctx, cast(sqe.args[0] >> 32 & 0x7fffffff), cast(sqe.args[0] & 0xffffffff), virtaddr(sqe.args[1]), cast(sqe.args[2]));

      case ioring_ops::poll_create:
        ctx.status = sys_poll_create(ring.process, &mut ctx, cast(sqe.args[0]));

      case ioring_ops::poll_add:
        ctx.status = sys_poll_add(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), cast(sqe.args[1] & 0x7fffffff), cast(sqe.args[2] & 0xffff), cast(sqe.args[3] & 0xffff), cast(sqe.args[4]), cast(sqe.args[5]));

      case ioring_ops::poll_remove:
        ctx.status = sys_poll_remove(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), cast(sqe.args[1] & 0x7fffffff), cast(sqe.args[2] & 0xffff));

      case ioring_ops::poll_wait:
        ctx.status = sys_poll_wait(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), virtaddr(sqe.args[1]), cast(sqe.args[2]), cast(sqe.args[3]));

      case ioring_ops::notify_create:
        ctx.status = sys_notify_create(ring.process, &mut ctx, cast(sqe.args[0]));

      case ioring_ops::notify_add:
        ctx.status = sys_notify_add(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), cast(sqe.args[1] >> 32 & 0x7fffffff), cast(sqe.args[1] & 0xffffffff), virtaddr(sqe.args[2]), cast(sqe.args[3]), cast(sqe.args[4]), cast(sqe.args[5]));

      case ioring_ops::notify_remove:
        ctx.status = sys_notify_remove(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), cast(sqe.args[1] >> 32 & 0x7fffffff), cast(sqe.args[1] & 0xffffffff), virtaddr(sqe.args[2]));

      case ioring_ops::event_create:
        ctx.status = sys_event_create(ring.process, &mut ctx, cast(sqe.args[0]), cast(sqe.args[1]), cast(sqe.args[2]));

      case ioring_ops::buffer_create:
        ctx.status = sys_buffer_create(ring.process, &mut ctx, virtaddr(sqe.args[0]), cast(sqe.args[1]), cast(sqe.args[2]));

      case ioring_ops::channel_create:
        ctx.status = sys_channel_create(ring.process, &mut ctx, virtaddr(sqe.args[0]), cast(sqe.args[1]));

      case ioring_ops::channel_read:
        ctx.status = sys_channel_read(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), virtaddr(sqe.args[1]), virtaddr(sqe.args[2]), virtaddr(sqe.args[3]), virtaddr(sqe.args[4]));

      case ioring_ops::channel_write:
        ctx.status = sys_channel_write(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), cast(sqe.args[1] & 0xffffffff), virtaddr(sqe.args[2]));

      case ioring_ops::channel_call:
        ctx.status = sys_channel_call(ring.process, &mut ctx, cast(sqe.args[0] & 0x7fffffff), virtaddr(sqe.args[1]), virtaddr(sqe.args[2]), virtaddr(sqe.args[3]), virtaddr(sqe.args[4]));

      else:
        ctx.status = result::not_supported;
    }
  }

  if (ctx.status == result::should_block)
    return should_block;

  ctx.wake_time = 0;
  ctx.blockers.clear();
  std::memset(&ctx.state, 0, sizeof(ctx.state));

  if (var result = ring.retire(sqe, ctx.status); !result)
    return should_block;

  if (sqe.flags & ioring_flags::fin == ioring_flags::fin)
    ctx.status = result::ok;

  return ok;
}

pub fn enqueue(io_ring mut &ring, ioring_sqe &sqe) -> void
{
  ring.pending.back.entries.push_back(sqe);

  if (ring.pending.len == 1 && ring.pending.back.entries.len == 1)
    workman.enqueue(&ring);

  if (sqe.flags & ioring_flags::fin == ioring_flags::fin)
    ring.pending.push_back();
}

pub fn retire(io_ring mut &ring, ioring_sqe &sqe, result status) -> result
{
  var header = cast<ioring_header mut *>(ring.region.host[0].addr.ptr);

  var tail = header.cq_tail;
  var index = cast<usize>(tail) & ring.cq_mask;
  var next = std::add_with_carry(tail, 1).0;

  if (std::sub_with_borrow(next, std::volatile_load(&header.cq_head)).0 > cast(ring.cq_mask + 1))
  {
    if (status == result::interrupted)
    {
      ring.waiters.wake_all();

      return ok;
    }

    return overflow;
  }

  var cqe = ioring_cqe(void);

  cqe.flags = 0;
  cqe.result = cast(status);
  cqe.user_data = sqe.user_data;

  if (var result = vm::memcpy(ring.region, ring.cq_base + index * sizeof<ioring_cqe>, &cqe, sizeof<ioring_cqe>); !result)
    std::panic("bad_cqe_write");

  std::atomic_store(&header.cq_tail, next);

  if (!ring.readable.ready)
    io::complete(&mut ring.readable.cb, 0);

  ring.waiters.wake_all();

  return ok;
}

pub fn submit(vfs::node mut *node, u32 max_submit) -> u32
{
  var mut &ring = *cast<io_ring mut *>(cast<uintptr>(node) - offsetof(io_ring::node));

  var submitted = 0;
  var outstanding = false;

  {
    var guard = std::lock_guard(&mut ring.lock);

    if (ring.region.host.empty)
      return 0;

    var header = cast<ioring_header mut *>(ring.region.host[0].addr.ptr);

    var head = header.sq_head;

    while (head != std::volatile_load(&header.sq_tail))
    {
      std::atomic_thread_fence(std::memory_order::acquire);

      if (submitted == max_submit)
        break;

      if (ring.pending.len > 16)
        break;

      if (ring.pending.back.entries.len > 8)
        break;

      if (ring.pending.back.status != result::should_block && ring.pending.back.entries.len != 0)
        break;

      var index = cast<usize>(head) & ring.sq_mask;

      var sqe = ioring_sqe(void);
      if (var result = vm::memcpy(&sqe, ring.region, ring.sq_base + index * sizeof<ioring_sqe>, sizeof<ioring_sqe>); !result)
        std::panic("bad_sqe_read");

      if (execute(&mut ring, &mut ring.pending.back, sqe) == result::should_block)
      {
        enqueue(&mut ring, sqe);
      }

      head = std::add_with_carry(head, 1).0;

      submitted += 1;
    }

    std::atomic_store(&header.sq_head, head);

    outstanding = (ring.pending.front.entries.len != 0);
  }

  if (outstanding)
    ioring::wake_from_stall();

  return submitted;
}

pub fn wait(vfs::node mut *node, u32 min_complete) -> u32
{
  var mut &ring = *cast<io_ring mut *>(cast<uintptr>(node) - offsetof(io_ring::node));

  var guard = std::lock_guard(&mut cpu::irqlock, &mut ring.lock);

  if (ring.pending.front.status != result::should_block && ring.pending.front.entries.len != 0)
    ioring::wake_from_stall();

  if (ring.region.host.empty)
    return 0;

  for (;;)
  {
    var header = cast<ioring_header mut *>(ring.region.host[0].addr.ptr);

    var completed = std::sub_with_borrow(header.cq_tail, header.cq_head).0;

    if (completed >= min_complete)
    {
      if (completed == min_complete)
      {
        io::reset(&mut ring.readable.cb);
        io::initiate(&mut ring.readable.cb, 1);
      }

      return completed;
    }

    ring.waiters.wait(&mut ring.lock);
  }
}

pub fn mmap(vfs::node mut *node, vm::virtaddr mut &addr, usize length, u64 offset, vm::protection prot, vm::usage use) -> vfs::result
{
  var mut &ring = *cast<io_ring mut *>(cast<uintptr>(node) - offsetof(io_ring::node));

  if (offset != 0)
    return invalid_argument;

  if (length != ring.region.length)
    return invalid_argument;

  if (process::current != ring.process)
    return not_permitted;

  var inset = ring.region.host[0].addr & PAGE_MASK;

  if (var result = vm::create_physical_region(vm::virtrange(addr, inset + length), ring.region.host, prot, use); !result)
    return cast(result);

  addr += inset;

  return ok;
}

pub fn shutdown(process mut *process) -> void
{
  while (process.iorings)
  {
    shutdown(process.iorings);
  }
}

pub fn shutdown(io_ring mut *ring) -> void
{
  var process = ring.process;

  var guard = std::lock_guard(&mut cpu::irqlock, &mut ring.lock);

  for (;;)
  {
    if (ring.pending.len == 1 && ring.pending.back.entries.empty)
      break;

    for (var mut &ctx : ring.pending)
      ctx.status = result::interrupted;

    ioring::wake_from_stall();

    ring.waiters.wait(&mut ring.lock);
  }

  vm::unlock(ring.region.host);
  ring.region.host.clear();
  ring.region.length = 0;

  process.lock.lock();

  var curr = &process.iorings;

  while (*curr != ring)
    curr = &curr.sibling;

  *curr = ring.sibling;

  process.lock.unlock();

  ring.sibling = null;
  ring.process = null;

  ring.unref();
}

fn destroy(vfs::node mut *node) -> void
{
  var ring = cast<io_ring mut *>(cast<uintptr>(node) - offsetof(io_ring::node));

  std::assert(!ring.next);
  std::assert(!ring.process);

  io_ring::allocator.free(ring);
}
