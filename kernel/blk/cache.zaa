//
// blk cache
//

import std.stdio;
import std.flat_hash_map;
import std.hash;
import vfs.fs;
import vfs.node;
import blk.cache;
import io;
import vm : PAGE_SIZE;
import cpu;
import heap;
import scheduler;
import mutex as _ : mutex;
import thread as _ : thread;
import process as _ : process;
import semaphore as _ : semaphore;
import slab : slab_allocator;
import support.rc : Rc;

const BLOCK_SIZE = 32768;
const BLOCK_SHIFT = std::ctz(BLOCK_SIZE);
const BLOCK_MASK = BLOCK_SIZE - 1;
const BLOCK_FLUSH_SIZE = 512;

struct cache_key
{
  vfs::node *node;
  u64 offset;

  cache_key(vfs::node *node, u64 offset)
    : node(node), offset(offset)
  {
  }

  fn ==(cache_key &, cache_key &) -> bool = default;

  cache_key(cache_key&) = default;
  ~cache_key() = default;
}

pub fn hash(var &hasher, cache_key &key) -> usize
{
  return hasher.combine(key.node, key.offset);
}

pub struct cache_entry
{
  enum state : int
  {
    uninit,
    allocating,
    allocated,
    loading,
    loaded,
  }

  state state;
  vm::virtrange allocation;
  vm::iovec iovec;
  io::response response;

  vfs::node mut *node;
  u64 offset;
  u64 dirty;

  cache_entry mut *next;
  cache_entry mut *prev;

  cache_entry mut *next_dirty;

  u8[0] reserved;

  i32 refcnt;

  fn allocator()
  {
    static allocator = #slab_allocator<cache_entry>();

    return &allocator;
  }

  fn create() -> cache_entry mut *
  {
    return allocator.allocate();
  }

  pub fn ref(this mut &) -> void
  {
    std::atomic_add(&this.refcnt, 1);
  }

  pub fn unref(this mut &) -> void
  {
    std::atomic_sub(&this.refcnt, 1);
  }

  pub cache_entry() = default;
  pub ~cache_entry() = default;
}

fn destroy(cache_entry mut *entry) -> void
{
  std::assert(entry.dirty == 0);
  std::assert(entry.response.outstanding == 0);
  std::assert(entry.next == null);
  std::assert(entry.prev == null);

  if (entry.state > cache_entry::state::allocated)
    entry.state = cache_entry::state::allocated;

  cache_entry::allocator.free(entry);
}

fn deallocate(cache_entry mut *entry) -> void
{
  vm::unlock(&mut entry.iovec);
  heap::munmap(entry.allocation);
  entry.allocation = vm::virtrange();

  entry.state = cache_entry::state::uninit;
}

pub struct block_ptr
{
  pub fn bool(this &) -> bool
  {
    return !!this.response;
  }

  pub fn ready(this &) -> bool
  {
    return this.response.ready;
  }

  pub fn result(this &) -> i32
  {
    return this.response.result;
  }

  pub fn ptr(this mut &, u64 offset) -> u8 mut *
  {
    return this.base + cast(offset & BLOCK_MASK);
  }

  pub fn base(this mut &) -> u8 mut *
  {
    return cast<u8 mut *>(this.entry.allocation.addr);
  }

  pub fn size(this mut &) -> usize
  {
    return BLOCK_SIZE;
  }

  pub fn state(this &&)
  {
    return &this.entry.state;
  }

  pub fn response(this &&)
  {
    return &this.entry.response;
  }

  Rc<cache_entry> entry;

  block_ptr(cache_entry mut *entry)
    : entry(entry)
  {
  }

  pub block_ptr() = default;
  pub block_ptr(block_ptr &&other) = default;
  pub fn =(block_ptr mut &lhs, block_ptr &&rhs) -> block_ptr mut & = default;
  pub ~block_ptr() = default;
}

pub fn wait(block_ptr mut &block) -> void
{
  block.response.wait();
}

pub fn wait_until(block_ptr mut &block, u64 abstime) -> bool
{
  if (var rc = block.response.wait_until(abstime); rc < 0)
    return false;

  return true;
}

struct block_cache
{
  mutex lock;

  std::flat_hash_map<cache_key, cache_entry mut *> entries;

  cache_entry mut *head;
  cache_entry mut *tail;

  usize blocks_allocated;

  thread mut *plumber;
  semaphore plumber_doorbell;
  cache_entry mut *drain;

  fn get(block_cache mut &pagecache, cache_key &key) -> block_ptr
  {
    var guard = std::lock_guard(&mut pagecache.lock);

    var j = pagecache.entries.find(key);

    if (j == pagecache.entries.end)
      j = pagecache.entries.insert(key, cache_entry::create()).0;

    pagecache.remove(pagecache.entries[j].value);
    pagecache.insert(pagecache.entries[j].value);

    return pagecache.entries[j].value;
  }

  fn insert(block_cache mut &pagecache, cache_entry mut *entry) -> void
  {
    entry.prev = pagecache.tail;

    if (entry.prev)
      entry.prev.next = entry;

    if (!pagecache.head)
      pagecache.head = entry;

    pagecache.tail = entry;
  }

  fn remove(block_cache mut &pagecache, cache_entry mut *entry) -> void
  {
    if (entry.next)
      entry.next.prev = entry.prev;

    if (entry.prev)
      entry.prev.next = entry.next;

    if (pagecache.head == entry)
      pagecache.head = entry.next;

    if (pagecache.tail == entry)
      pagecache.tail = entry.prev;

    entry.next = null;
    entry.prev = null;
  }

  fn instance() -> block_cache mut &
  {
    static instance = #block_cache();

    return &instance;
  }

  block_cache() = default;
  block_cache(#block_cache&) = default;
  ~block_cache() = default;
}

fn pagecache() -> block_cache mut &
{
  return &block_cache::instance;
}

pub fn cache_memory_used() -> usize
{
  return std::atomic_load(&pagecache.blocks_allocated) * BLOCK_SIZE;
}

pub fn fetch(Rc<vfs::node> &node, u64 offset) -> block_ptr
{
  offset &= ~BLOCK_MASK;

  var block = pagecache.get(cache_key(node, offset));

  if (std::atomic_cmpxchg_strong(&block.state, cache_entry::state::uninit, cache_entry::state::allocating))
  {
    block.entry.allocation = heap::mmap(BLOCK_SIZE);

    if (var result = vm::lock(block.entry.allocation, vm::protection::readwrite, &mut block.entry.iovec); !result)
      std::panic("unable to lock block");

    std::atomic_add(&pagecache.blocks_allocated, 1);

    block.state = cache_entry::state::allocated;
  }

  if (std::atomic_cmpxchg_strong(&block.state, cache_entry::state::allocated, cache_entry::state::loading))
  {
    block.entry.node = node;
    block.entry.offset = offset;
    block.entry.response = io::response();

    if (var result = node.file_operations.read(node, &mut block.response.cb, offset, block.entry.iovec, 0, block.size); !result)
      io::cancel(&mut block.response.cb, result);

    block.state = cache_entry::state::loaded;
  }

  while (std::volatile_load(&block.state) < cache_entry::state::loaded)
    scheduler::sleep_yield();

  return block;
}

pub fn commit(block_ptr mut &block, void *data, usize len) -> void
{
  var dirty = 0;

  for (var i = (cast<u8*>(data) - block.base) / BLOCK_FLUSH_SIZE; i < std::ceil_div(cast<u8*>(data) - block.base + len, BLOCK_FLUSH_SIZE); i += 1)
  {
    dirty |= 1 << i;
  }

  if (dirty == 0)
    return;

  if (std::atomic_or(&block.entry.dirty, dirty) == 0)
  {
    block.entry.ref();
    block.entry.node.ref();

    for (;;)
    {
      block.entry.next_dirty = std::volatile_load(&pagecache.drain);

      if (std::atomic_cmpxchg_weak(&pagecache.drain, block.entry.next_dirty, &*block.entry))
        break;
    }

    pagecache.plumber_doorbell.release();
  }
}

pub fn purge(vfs::node *node) -> void
{
  var guard = std::lock_guard(&mut pagecache.lock);

  for (var entry = pagecache.head; entry; )
  {
    var next = entry.next;

    if (entry.node == node)
    {
      pagecache.entries.erase(cache_key(entry.node, entry.offset));
      pagecache.remove(entry);
      destroy(entry);
    }

    entry = next;
  }
}

pub fn reap() -> void
{
  if (pagecache.lock.try_lock())
  {
    var count = 0;

    for (var entry = pagecache.head; entry; )
    {
      var next = entry.next;

      if (entry.refcnt == 0)
      {
        var refcnt = entry.response.refcnt;

        entry.iovec.foreach_region_in(0, BLOCK_SIZE, |region| {
          for (var &page : vm::pages_for_range(region))
            refcnt += page.refcnt;
        });

        if (refcnt - 2*(BLOCK_SIZE/PAGE_SIZE) - 1 == 0)
        {
          pagecache.entries.erase(cache_key(entry.node, entry.offset));
          pagecache.remove(entry);
          deallocate(entry);
          destroy(entry);

          std::atomic_sub(&pagecache.blocks_allocated, 1);

          if (++count == 4)
            break;
        }
      }

      entry = next;
    }

    pagecache.lock.unlock();
  }

  cache_entry::allocator.reap();
}

fn plumber(void*) -> void
{
  for (;;)
  {
    scheduler::sleep_until(cpu::system_time + 1_000_000_000);

    for (var entry = std::atomic_xchg(&pagecache.drain, null); entry; )
    {
      var node = entry.node;
      var next = entry.next_dirty;
      var dirty = std::atomic_xchg(&entry.dirty, 0);
      var response = io::response();
      var position = entry.offset;

      for (var offset = 0; dirty != 0; )
      {
        var bytes = BLOCK_FLUSH_SIZE;

        if (dirty & 1 != 0)
        {
          while (dirty & 0x2 != 0)
          {
            dirty >>= 1;
            bytes += BLOCK_FLUSH_SIZE;
          }

          if (var result = node.file_operations.write(node, &mut response.cb, position, entry.iovec, offset, bytes); !result)
            std::print("plumber: unable to initiate write back - ", result);
        }

        dirty >>= 1;
        offset += bytes;
        position += cast(bytes);
      }

      if (response.wait(); !response)
        std::print("plumber: unable to complete write back - ", response.result);

      if (response)
        std::print("plumber: writeback ", response.result, " bytes");

      entry.unref();
      node.unref();

      entry = next;
    }

    pagecache.plumber_doorbell.wait();
  }
}

pub fn spawn_plumber() -> void
{
  pagecache.plumber = create_thread(process::current, &cast<(void mut *) -> void>(plumber), null);

  scheduler::enqueue(pagecache.plumber);
}
